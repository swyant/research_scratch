{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to investigate using PCA before doing the kmeans distance. Maybe compare with random projection (which should be worth although aren't there guarantees on distance preservation that may be relevant? idk, totally out of my league here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "\n",
    "using PotentialLearning, InteratomicPotentials\n",
    "using Unitful\n",
    "using Random\n",
    "using AtomsBase\n",
    "using DelimitedFiles\n",
    "using Statistics: mean, var\n",
    "using StatsBase\n",
    "using Clustering, Distances\n",
    "using Trapz\n",
    "using LinearAlgebra: Symmetric, eigen, mul!, svd, cond, dot, norm\n",
    "\n",
    "using MultivariateStats, StatsAPI\n",
    "\n",
    "#using CairoMakie CairoMakie.activate!()\n",
    "using GLMakie; GLMakie.activate!(inline=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace = ACE(species           = [:C,:H,:O,:N],\n",
    "          body_order        = 3,\n",
    "          polynomial_degree = 10,\n",
    "          wL                = 2.0,\n",
    "          csp               = 1.0,\n",
    "          r0                = 1.43,\n",
    "          rcutoff           = 4.4 )\n",
    "lb = LBasisPotential(ace)\n",
    "length(ace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_file = \"../files/QM9/qm9_fullset_alldata.xyz\"\n",
    "raw_data = load_data(qm9_file, ExtXYZ(u\"eV\", u\"Å\"))\n",
    "raw_data = DataSet([config for config in raw_data if !(:F in atomic_symbol(get_system(config)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_train = 120_001\n",
    "master_perm_idxs = readdlm(\"./primary_permutation.txt\", Int64)\n",
    "possible_training_idxs = master_perm_idxs[1:max_num_train]\n",
    "possible_test_idxs = master_perm_idxs[max_num_train+1:end]\n",
    "\n",
    "num_train = 40_000\n",
    "train_idxs = possible_training_idxs[1:num_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.β .= readdlm(\"qm9_4elem_3body_poly10_fit40K.txt\", Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etest_ref = get_all_energies(raw_data[possible_test_idxs])\n",
    "\n",
    "etest_local_descrs = compute_local_descriptors(raw_data[possible_test_idxs],lb.basis)\n",
    "ds_test = DataSet(raw_data[possible_test_idxs] .+ etest_local_descrs)\n",
    "etest_pred = get_all_energies(ds_test,lb)\n",
    "\n",
    "num_atoms_test = length.(get_system.(raw_data[possible_test_idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show e_mae, e_rmse, e_rsq = calc_metrics(etest_pred,etest_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_mean_features(ds)\n",
    "    mean_feature_perconfig = Vector{Float64}[]\n",
    "    for (i,config) in enumerate(ds)\n",
    "        if i % 100 == 0\n",
    "            println(i)\n",
    "        end\n",
    "        mean_feature = mean(InteratomicPotentials.compute_local_descriptors(get_system(config), lb.basis))\n",
    "        push!(mean_feature_perconfig,mean_feature)\n",
    "    end\n",
    "\n",
    "    reduce(hcat,mean_feature_perconfig)\n",
    "end\n",
    "\n",
    "# can either average over the distances between centers, or find the minimum\n",
    "# could also vary the distance here...\n",
    "function heuristic_uncertainty1(mean_feature_vec, km)\n",
    "    #dist = mean(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "    dist = minimum(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "end\n",
    "\n",
    "function heuristic_uncertainty2(mean_feature_vec, km)\n",
    "    dist = mean(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "    #dist = minimum(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "end\n",
    "\n",
    "function cosine_similarity(x,y)\n",
    "    return dot(x,y)/((norm(x)*norm(y)))\n",
    "end\n",
    "\n",
    "function heuristic_uncertainty3(mean_feature_vec, km)\n",
    "    dist = 1 - maximum(mapslices(x->cosine_similarity(mean_feature_vec,x), km.centers, dims=1))\n",
    "    return dist\n",
    "end\n",
    "\n",
    "# useless because cosine similarity can be negative\n",
    "function heuristic_uncertainty4(mean_feature_vec, km)\n",
    "    dist = 1 - mean(mapslices(x->cosine_similarity(mean_feature_vec,x), km.centers, dims=1))\n",
    "    return dist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_features = compute_mean_features(raw_data[train_idxs])\n",
    "mean_test_features  = compute_mean_features(raw_data[possible_test_idxs])\n",
    "\n",
    "dt = StatsBase.fit(ZScoreTransform, mean_train_features, dims=2)\n",
    "std_mean_train_features = StatsBase.transform(dt,mean_train_features)\n",
    "std_mean_test_features = StatsBase.transform(dt,mean_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = std_mean_train_features\n",
    "x[:,1] * x[:,1]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual, non-SVD implementation, doing it myself here just so I know what's happening\n",
    "# data is assumed to be n x d\n",
    "function my_incomplete_pca(data)\n",
    "    d,n = size(data)\n",
    "    XXt = zeros((d,d))\n",
    "    for col in eachcol(data)\n",
    "        XXt .+= col * col'\n",
    "    end\n",
    "\n",
    "    mean_XXt = XXt ./ n\n",
    "end\n",
    "\n",
    "function my_incomplete_pca2(data)\n",
    "    d,n = size(data)\n",
    "    XXt = zeros((d,d))\n",
    "    # Pre-allocate the temporary vector for outer product\n",
    "    temp = Vector{Float64}(undef, d)\n",
    "\n",
    "    for col in eachcol(data)\n",
    "        # In-place outer product accumulation\n",
    "        @views for i in 1:d\n",
    "            temp[i] = col[i]\n",
    "            for j in i:d  # Use symmetry to reduce computations\n",
    "                XXt[i,j] += temp[i] * col[j]\n",
    "            end\n",
    "        end\n",
    "        # Fill in the lower triangle using symmetry\n",
    "        for i in 1:d\n",
    "            for j in 1:i-1\n",
    "                XXt[i,j] = XXt[j,i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    XXt ./= n\n",
    "    return XXt\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time ver1 = my_incomplete_pca(x[:,1:1000])\n",
    "@time ver2 = my_incomplete_pca2(x[:,1:1000])\n",
    "ver1 ≈ ver2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two cells were just clarify that the below doesn't work, though actually... I should be able to do a matrix product. yea nevermind this works too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_d, my_n = size(x)\n",
    "my_XXT = Matrix{Float64}(undef,my_d,my_d)\n",
    "mul!(my_XXT, x[:,1:1000], x[:,1:1000]')\n",
    "my_XXT ./= my_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x = x[:,1:1000]\n",
    "my_xt = x[:,1:1000]'\n",
    "my_x * my_xt ./ my_n # ahh ok so this does match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_pca(data)\n",
    "    d,n = size(data)\n",
    "    XXt = zeros((d,d))\n",
    "    # Pre-allocate the temporary vector for outer product\n",
    "    temp = Vector{Float64}(undef, d)\n",
    "\n",
    "    for col in eachcol(data)\n",
    "        # In-place outer product accumulation\n",
    "        @views for i in 1:d\n",
    "            temp[i] = col[i]\n",
    "            for j in i:d  # Use symmetry to reduce computations\n",
    "                XXt[i,j] += temp[i] * col[j]\n",
    "            end\n",
    "        end\n",
    "        # Fill in the lower triangle using symmetry\n",
    "        for i in 1:d\n",
    "            for j in 1:i-1\n",
    "                XXt[i,j] = XXt[j,i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    mean_XXt = XXt ./ n\n",
    "\n",
    "    @show cond(mean_XXt)\n",
    "\n",
    "\n",
    "    eigen(Symmetric(mean_XXt))\n",
    "end\n",
    "\n",
    "function my_pca2(data)\n",
    "    d,n = size(data)\n",
    "    XXt = zeros((d,d))\n",
    "\n",
    "    mul!(XXt, data, data')\n",
    "\n",
    "    mean_XXt = XXt ./ n\n",
    "\n",
    "    @show cond(mean_XXt)\n",
    "\n",
    "    eigen(Symmetric(mean_XXt))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambdas, evecs = my_pca(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_lambda, alt_evecs = my_pca2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(stdout, \"text/plain\",lambdas[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_lambda1 = [ val > 0 ? val : eps() for val in lambdas[end:-1:1]]\n",
    "plot_lambda2 = [  val > 0 ? val : eps() for val in alt_lambda[end:-1:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = plot(log10.(plot_lambda1))\n",
    "plot(log10.(plot_lambda2))\n",
    "#display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function svd_pca(data)\n",
    "    #F = svd(data; full=true) # takes too long\n",
    "    F = svd(data)\n",
    "    evecs = F.V\n",
    "    svs = F.S\n",
    "    #svs, evecs\n",
    "    F\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambdas2, evecs2 = svd_pca(x)\n",
    "F = svd_pca(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = StatsAPI.fit(MultivariateStats.PCA, x; mean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2 = StatsAPI.fit(MultivariateStats.PCA,x; method=:svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1.proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_std_train_features = StatsAPI.predict(M1, std_mean_train_features)\n",
    "pca_std_test_features = StatsAPI.predict(M1, std_mean_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pca_std_train_features\n",
    "#num_neighbors = 20\n",
    "km = kmeans(train_features, 20, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "km_10 = kmeans(train_features, 10, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "km_50  = kmeans(train_features, 50, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "\n",
    "km_200 = kmeans(train_features, 200, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "km_2000 = kmeans(train_features, 2000, distance=Distances.Euclidean(), rng=Xoshiro(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_distances1 = mapslices(vec -> heuristic_uncertainty1(vec, km), pca_std_test_features, dims=1)\n",
    "test_feature_distances2 = mapslices(vec -> heuristic_uncertainty2(vec, km), pca_std_test_features, dims=1)\n",
    "\n",
    "test_feature_distances3 = mapslices(vec -> heuristic_uncertainty1(vec, km_10), pca_std_test_features, dims=1)\n",
    "test_feature_distances4 = mapslices(vec -> heuristic_uncertainty1(vec, km_50), pca_std_test_features, dims=1)\n",
    "\n",
    "\n",
    "test_feature_distances5 = mapslices(vec -> heuristic_uncertainty3(vec, km_50), pca_std_test_features, dims=1)\n",
    "test_feature_distances6 = mapslices(vec -> heuristic_uncertainty3(vec, km_50), pca_std_test_features, dims=1)\n",
    "\n",
    "\n",
    "test_feature_distances7 = mapslices(vec -> heuristic_uncertainty3(vec, km_200), pca_std_test_features, dims=1)\n",
    "test_feature_distances8 = mapslices(vec -> heuristic_uncertainty1(vec, km_200), pca_std_test_features, dims=1)\n",
    "\n",
    "test_feature_distances9 = mapslices(vec -> heuristic_uncertainty3(vec, km_2000), pca_std_test_features, dims=1)\n",
    "test_feature_distances10 = mapslices(vec -> heuristic_uncertainty1(vec, km_2000), pca_std_test_features, dims=1)\n",
    "\n",
    "test_feature_distances11 = mapslices(vec -> heuristic_uncertainty4(vec, km_2000), pca_std_test_features, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in eachcol(km_2000.centers)\n",
    "    @show cosine_similarity(pca_std_test_features[:,1],center)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_distances = test_feature_distances11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(test_feature_distances[1,:], bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction_calib = 0.1\n",
    "peratom = true\n",
    "alpha = 0.05\n",
    "num_calib = floor(Int64, fraction_calib*length(possible_test_idxs))\n",
    "num_test = length(possible_test_idxs) - num_calib\n",
    "\n",
    "#idxs_wrt_test = Random.randperm(length(possible_test_idxs))\n",
    "idxs_wrt_test = collect(1:length(possible_test_idxs))\n",
    "\n",
    "calib_idxs_wrt_test = idxs_wrt_test[1:num_calib]\n",
    "test_idxs_wrt_test = idxs_wrt_test[num_calib+1:end]\n",
    "\n",
    "if !peratom\n",
    "    calib_scores = abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test]) ./ test_feature_distances[calib_idxs_wrt_test]\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test])\n",
    "\n",
    "else\n",
    "    calib_scores = ( abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test])\n",
    "                    ./ num_atoms_test[calib_idxs_wrt_test] ./ test_feature_distances[calib_idxs_wrt_test] )\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./ num_atoms_test[test_idxs_wrt_test]\n",
    "\n",
    "end\n",
    "\n",
    "q_hat = quantile(calib_scores, ceil((num_calib+1)*(1-alpha))/num_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qhat_scores = q_hat*test_feature_distances[test_idxs_wrt_test]\n",
    "coverage = sum(test_abs_residuals .> qhat_scores) / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = (etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./num_atoms_test[test_idxs_wrt_test]\n",
    "#hist(abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./num_atoms_test[test_idxs_wrt_test],bins=100)\n",
    "hist(test_residuals,bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function uncertainty_vs_residuals(uncertainty, residuals;\n",
    "    title  =\"Uncertainty vs. residuals\",\n",
    "    xlabel = \"Distance\",\n",
    "    ylabel = \"Residuals\",\n",
    "    figsize = (600,600))\n",
    "\n",
    "    fig = Figure(size=(600,600))\n",
    "    ax  = Axis(fig[1,1],\n",
    "    title=\"Residuals vs distances\",\n",
    "    xlabel=\"Distances\",\n",
    "    ylabel=\"Residuals\",\n",
    "    #limits=(0,100,-0.01,0.03))\n",
    "    limits=(0,1,-0.01,0.03))\n",
    "\n",
    "    hlines!(ax, 0.0, color=:red, linestyle=:dash)\n",
    "\n",
    "    scatter!(ax, uncertainty, residuals, markersize=5)\n",
    "\n",
    "    #ax.aspect=DataAspect()\n",
    "    fig\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#scatter(test_feature_distances[test_idxs_wrt_test], test_abs_residuals)\n",
    "uncertainty_vs_residuals(test_feature_distances[test_idxs_wrt_test], test_abs_residuals) # idk why this isn't plotting correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = test_feature_distances[test_idxs_wrt_test]\n",
    "yvals = test_abs_residuals\n",
    "A = hcat(ones(length(xvals)), xvals)\n",
    "coeffs = A \\ yvals\n",
    "@show coeffs\n",
    "y_pred = coeffs[2] .*xvals .+ coeffs[1]\n",
    "\n",
    "ymean = mean(yvals)\n",
    "ss_total = sum((yvals .- ymean).^2)\n",
    "ss_residual = sum((yvals .- y_pred).^2)\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "# not very meaningful for bounded between 0 and 1 ? I guess it's still should be fine but the R2 is much worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_res = test_abs_residuals\n",
    "@show length(abs_res)\n",
    "for bin_start in 0.000:0.001:0.015\n",
    "    low = bin_start\n",
    "    high = bin_start + 0.001\n",
    "    idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "    local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "    println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "end\n",
    "low = 0.012\n",
    "high = 0.02\n",
    "idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "\n",
    "local_coverage = 1 - sum(abs_res .> qhat_scores)/length(abs_res)\n",
    "println(\"overall coverage is $(local_coverage)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness = 2*mean(qhat_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var(qhat_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(qhat_scores,bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_complements = collect(range(0.01,0.99,step=0.01))\n",
    "alpha_refs = 1 .- alpha_complements\n",
    "\n",
    "alpha_refs = collect(range(0.01,0.99,step=0.01))\n",
    "\n",
    "predicted_alphas = Float64[]\n",
    "#for ac in alpha_complements\n",
    "#    alpha = 1-ac\n",
    "for alpha in alpha_refs\n",
    "    qh = quantile(calib_scores, clamp(ceil((num_calib+1)*(1-alpha))/num_calib, 0.0, 1.0))\n",
    "\n",
    "    qh_scores = qh*test_feature_distances[test_idxs_wrt_test]\n",
    "    predicted_alpha = sum(test_abs_residuals .> qh_scores) / num_test\n",
    "    push!(predicted_alphas, predicted_alpha)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_calibration_plot(alpha_refs,predicted_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_miscalibration_area(expected_ps, observed_ps)\n",
    "    area = 0.0\n",
    "    #for i in 2:length(expected_ps)-1\n",
    "    #    trap = abs(trapz(expected_ps[i-1:i+1], observed_ps[i-1:i+1]) -\n",
    "    #             trapz(expected_ps[i-1:i+1], expected_ps[i-1:i+1]))\n",
    "    for i in 2:length(expected_ps)\n",
    "        trap = abs(trapz(expected_ps[i-1:i], observed_ps[i-1:i]) -\n",
    "                 trapz(expected_ps[i-1:i], expected_ps[i-1:i]))\n",
    "        area += trap\n",
    "    end\n",
    "    area\n",
    "end\n",
    "\n",
    "# converted from Medford jupyter notebook via Claude\n",
    "function make_calibration_plot(expected_ps, observed_ps; width=600)\n",
    "    # Convert to percentages\n",
    "    expected_ps = expected_ps .* 100\n",
    "    observed_ps = observed_ps .* 100\n",
    "\n",
    "    fig = Figure(resolution=(width, width))\n",
    "    ax = Axis(fig[1, 1],\n",
    "        aspect=DataAspect(),\n",
    "        xlabel=\"Expected conf. level\",\n",
    "        ylabel=\"Observed conf. level\",\n",
    "        limits=(0, 100, 0, 100)\n",
    "    )\n",
    "\n",
    "    # Main line\n",
    "    lines!(ax, 1.0 .- expected_ps, observed_ps)\n",
    "\n",
    "    # Diagonal reference line\n",
    "    lines!(ax, 1.0 .-expected_ps, 1.0 .-expected_ps, linestyle=:dash, alpha=0.4)\n",
    "\n",
    "    # Filled area between curves\n",
    "    band!(ax, expected_ps, expected_ps, observed_ps, color=(:blue, 0.2))\n",
    "\n",
    "    # Configure ticks - approximately 4 ticks on each axis\n",
    "    ax.xticks = 0:10:100\n",
    "    ax.yticks = 0:10:100\n",
    "\n",
    "    # Add percentage signs to ticks\n",
    "    ax.xtickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "    ax.ytickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "\n",
    "    ## Add text for miscalibration area\n",
    "    #text!(ax, \"miscalc. area = $(round(area, digits=3))\",\n",
    "    #    position=(8, 2),\n",
    "    #    align=(:left, :bottom)\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "end\n",
    "\n",
    "# Claude\n",
    "function parity_plot(etest_ref, etest_pred, qhat_scored;\n",
    "                     title=\"Parity Plot\",\n",
    "                     xlabel=\"Reference Values\",\n",
    "                     ylabel=\"Predicted Values\",\n",
    "                     figsize=(600, 600))\n",
    "    # Create figure and axis\n",
    "    fig = Figure(size=figsize)\n",
    "    ax = Axis(fig[1, 1],\n",
    "    title=title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    limits = (-5.0,-4.0,-5.0,-4.0))\n",
    "\n",
    "    # Calculate min and max for setting plot limits\n",
    "    min_val = min(minimum(etest_pred), minimum(etest_ref))\n",
    "    max_val = max(maximum(etest_pred), maximum(etest_ref))\n",
    "\n",
    "    # Add diagonal reference line\n",
    "    lines!(ax, [min_val, max_val], [min_val, max_val],\n",
    "    color=:red,\n",
    "    linestyle=:dash,\n",
    "    label=\"Perfect Prediction\")\n",
    "\n",
    "    # Plot scatter with error bars\n",
    "    errorbars!(ax, etest_ref, etest_pred, qhat_scored,\n",
    "    whiskerwidth=1,  # Width of error bar caps\n",
    "    color=:cyan3)\n",
    "\n",
    "    # Scatter plot of points\n",
    "    scatter!(ax, etest_ref, etest_pred,\n",
    "    color=:teal,\n",
    "    markersize=10)\n",
    "\n",
    "    # Set equal aspect ratio\n",
    "    #ax.aspect = DataAspect()\n",
    "\n",
    "    # Add legend\n",
    "    axislegend(ax)\n",
    "\n",
    "    return fig\n",
    "end\n",
    "\n",
    "function uncertainty_vs_residuals(uncertainty, residuals;\n",
    "                                 title  =\"Uncertainty vs. residuals\",\n",
    "                                 xlabel = \"Distance\",\n",
    "                                 ylabel = \"Residuals\",\n",
    "                                 figsize = (600,600))\n",
    "    fig = Figure(size=figsize)\n",
    "    ax  = Axis(fig[1,1],\n",
    "               title=title,\n",
    "               xlabel=xlabel,\n",
    "               ylabel=ylabel)\n",
    "\n",
    "    hlines!(ax, 0.0, color=:red, linestyle=:dash)\n",
    "\n",
    "    scatter!(ax, uncertainty, residuals, markersize=1)\n",
    "\n",
    "    #ax.aspect=DataAspect()\n",
    "    fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_miscalibration_area(alpha_refs, predicted_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
