{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "\n",
    "using PotentialLearning, InteratomicPotentials\n",
    "using Unitful\n",
    "using Random\n",
    "using AtomsBase\n",
    "using DelimitedFiles\n",
    "using Statistics: mean, var\n",
    "using StatsBase\n",
    "using Clustering, Distances, NearestNeighbors\n",
    "using Trapz\n",
    "using LinearAlgebra: Symmetric, eigen, mul!, svd, cond, dot, norm\n",
    "\n",
    "using MultivariateStats, StatsAPI\n",
    "\n",
    "using JLD2\n",
    "\n",
    "#using CairoMakie CairoMakie.activate!()\n",
    "using GLMakie; GLMakie.activate!(inline=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace = ACE(species           = [:C,:H,:O,:N],\n",
    "          body_order        = 3,\n",
    "          polynomial_degree = 10,\n",
    "          wL                = 2.0,\n",
    "          csp               = 1.0,\n",
    "          r0                = 1.43,\n",
    "          rcutoff           = 4.4 )\n",
    "lb = LBasisPotential(ace)\n",
    "length(ace)\n",
    "\n",
    "qm9_file = \"../files/QM9/qm9_fullset_alldata.xyz\"\n",
    "raw_data = load_data(qm9_file, ExtXYZ(u\"eV\", u\"Å\"))\n",
    "raw_data = DataSet([config for config in raw_data if !(:F in atomic_symbol(get_system(config)))])\n",
    "\n",
    "max_num_train = 120_001\n",
    "master_perm_idxs = readdlm(\"./primary_permutation.txt\", Int64)\n",
    "possible_training_idxs = master_perm_idxs[1:max_num_train]\n",
    "possible_test_idxs = master_perm_idxs[max_num_train+1:end]\n",
    "\n",
    "num_train = 40_000\n",
    "train_idxs = possible_training_idxs[1:num_train]\n",
    "\n",
    "lb.β .= readdlm(\"qm9_4elem_3body_poly10_fit40K.txt\", Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etest_ref = get_all_energies(raw_data[possible_test_idxs])\n",
    "\n",
    "etest_local_descrs = compute_local_descriptors(raw_data[possible_test_idxs],lb.basis)\n",
    "ds_test = DataSet(raw_data[possible_test_idxs] .+ etest_local_descrs)\n",
    "etest_pred = get_all_energies(ds_test,lb)\n",
    "\n",
    "num_atoms_test = length.(get_system.(raw_data[possible_test_idxs]))\n",
    "\n",
    "@show e_mae, e_rmse, e_rsq = calc_metrics(etest_pred./num_atoms_test,etest_ref./num_atoms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etrain_ref = get_all_energies(raw_data[train_idxs])\n",
    "\n",
    "etrain_local_descrs = compute_local_descriptors(raw_data[train_idxs],lb.basis)\n",
    "ds_train = DataSet(raw_data[train_idxs] .+ etrain_local_descrs)\n",
    "etrain_pred = get_all_energies(ds_train,lb)\n",
    "\n",
    "num_atoms_train = length.(get_system.(raw_data[train_idxs]))\n",
    "\n",
    "@show etrain_mae, etrain_rmse, etrain_rsq = calc_metrics(etrain_pred./num_atoms_train,etrain_ref./num_atoms_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So really, I should have a separate validation data set (maybe I can use the calibration dataset?) to compute the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_residuals = (etrain_pred .- etrain_ref) ./ num_atoms_train\n",
    "trainset_std = sqrt(var(train_residuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_mean_features(ds)\n",
    "    mean_feature_perconfig = Vector{Float64}[]\n",
    "    for (i,config) in enumerate(ds)\n",
    "        if i % 100 == 0\n",
    "            println(i)\n",
    "        end\n",
    "        mean_feature = mean(InteratomicPotentials.compute_local_descriptors(get_system(config), lb.basis))\n",
    "        push!(mean_feature_perconfig,mean_feature)\n",
    "    end\n",
    "\n",
    "    reduce(hcat,mean_feature_perconfig)\n",
    "end\n",
    "\n",
    "function normdists2centers(feature_vec, km)\n",
    "    dists = mapslices(x->Distances.euclidean(feature_vec,x), km.centers, dims=1)\n",
    "    normed_dists = dists ./ sum(dists)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_features = compute_mean_features(raw_data[train_idxs])\n",
    "mean_test_features  = compute_mean_features(raw_data[possible_test_idxs])\n",
    "\n",
    "dt = StatsBase.fit(ZScoreTransform, mean_train_features, dims=2)\n",
    "std_mean_train_features = StatsBase.transform(dt,mean_train_features)\n",
    "std_mean_test_features = StatsBase.transform(dt,mean_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty = trainset_std\n",
    "\n",
    "fraction_calib = 0.1\n",
    "peratom = true\n",
    "alpha = 0.05\n",
    "num_calib = floor(Int64, fraction_calib*length(possible_test_idxs))\n",
    "num_test = length(possible_test_idxs) - num_calib\n",
    "\n",
    "#idxs_wrt_test = Random.randperm(length(possible_test_idxs))\n",
    "idxs_wrt_test = collect(1:length(possible_test_idxs))\n",
    "\n",
    "calib_idxs_wrt_test = idxs_wrt_test[1:num_calib]\n",
    "test_idxs_wrt_test = idxs_wrt_test[num_calib+1:end]\n",
    "\n",
    "if !peratom\n",
    "    calib_scores = abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test]) ./ uncertainty\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test])\n",
    "\n",
    "else\n",
    "    calib_scores = ( abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test])\n",
    "                    ./ num_atoms_test[calib_idxs_wrt_test] ./ uncertainty )\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./ num_atoms_test[test_idxs_wrt_test]\n",
    "\n",
    "end\n",
    "\n",
    "q_hat = quantile(calib_scores, ceil((num_calib+1)*(1-alpha))/num_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qhat_scores = q_hat*stdev*ones(num_test)\n",
    "coverage = sum(test_abs_residuals .> qhat_scores) / num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.06161905949856555 # randomly not good here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_res = test_abs_residuals\n",
    "@show length(abs_res)\n",
    "for bin_start in 0.000:0.001:0.015\n",
    "    low = bin_start\n",
    "    high = bin_start + 0.001\n",
    "    idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "    local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "    println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "end\n",
    "low = 0.012\n",
    "high = 0.02\n",
    "idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "\n",
    "local_coverage = 1 - sum(abs_res .> qhat_scores)/length(abs_res)\n",
    "println(\"overall coverage is $(local_coverage)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "length(abs_res) = 8017\n",
    "0.0-0.001 : 2512 configs with coverage 1.0\n",
    "0.001-0.002 : 1896 configs with coverage 1.0\n",
    "0.002-0.003 : 1222 configs with coverage 1.0\n",
    "0.003-0.004 : 805 configs with coverage 1.0\n",
    "0.004-0.005 : 550 configs with coverage 1.0\n",
    "0.005-0.006 : 337 configs with coverage 1.0\n",
    "0.006-0.007 : 185 configs with coverage 1.0\n",
    "0.007-0.008 : 147 configs with coverage 0.108843537414966\n",
    "0.008-0.009000000000000001 : 108 configs with coverage 0.0\n",
    "0.009-0.009999999999999998 : 61 configs with coverage 0.0\n",
    "0.01-0.011 : 44 configs with coverage 0.0\n",
    "0.011-0.012 : 40 configs with coverage 0.0\n",
    "0.012-0.013000000000000001 : 16 configs with coverage 0.0\n",
    "0.013-0.013999999999999999 : 16 configs with coverage 0.0\n",
    "0.014-0.015 : 18 configs with coverage 0.0\n",
    "0.015-0.016 : 12 configs with coverage 0.0\n",
    "0.012-0.02 : 87 configs with coverage 0.0\n",
    "overall coverage is 0.9383809405014345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_complements = collect(range(0.01,0.99,step=0.01))\n",
    "alpha_refs = 1 .- alpha_complements\n",
    "\n",
    "alpha_refs = collect(range(0.01,0.99,step=0.01))\n",
    "\n",
    "predicted_alphas = Float64[]\n",
    "#for ac in alpha_complements\n",
    "#    alpha = 1-ac\n",
    "for alpha in alpha_refs\n",
    "    qh = quantile(calib_scores, clamp(ceil((num_calib+1)*(1-alpha))/num_calib, 0.0, 1.0))\n",
    "\n",
    "    qh_scores = qh*stdev*ones(num_test)\n",
    "    predicted_alpha = sum(test_abs_residuals .> qh_scores) / num_test\n",
    "    push!(predicted_alphas, predicted_alpha)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_miscalibration_area(expected_ps, observed_ps)\n",
    "    area = 0.0\n",
    "    #for i in 2:length(expected_ps)-1\n",
    "    #    trap = abs(trapz(expected_ps[i-1:i+1], observed_ps[i-1:i+1]) -\n",
    "    #             trapz(expected_ps[i-1:i+1], expected_ps[i-1:i+1]))\n",
    "    for i in 2:length(expected_ps)\n",
    "        trap = abs(trapz(expected_ps[i-1:i], observed_ps[i-1:i]) -\n",
    "                 trapz(expected_ps[i-1:i], expected_ps[i-1:i]))\n",
    "        area += trap\n",
    "    end\n",
    "    area\n",
    "end\n",
    "\n",
    "# converted from Medford jupyter notebook via Claude\n",
    "function make_calibration_plot(expected_ps, observed_ps; width=600)\n",
    "    # Convert to percentages\n",
    "    expected_ps = expected_ps .* 100\n",
    "    observed_ps = observed_ps .* 100\n",
    "\n",
    "    fig = Figure(resolution=(width, width))\n",
    "    ax = Axis(fig[1, 1],\n",
    "        aspect=DataAspect(),\n",
    "        xlabel=\"Expected conf. level\",\n",
    "        ylabel=\"Observed conf. level\",\n",
    "        limits=(0, 100, 0, 100)\n",
    "    )\n",
    "\n",
    "    # Main line\n",
    "    lines!(ax, 1.0 .- expected_ps, observed_ps)\n",
    "\n",
    "    # Diagonal reference line\n",
    "    lines!(ax, 1.0 .-expected_ps, 1.0 .-expected_ps, linestyle=:dash, alpha=0.4)\n",
    "\n",
    "    # Filled area between curves\n",
    "    band!(ax, expected_ps, expected_ps, observed_ps, color=(:blue, 0.2))\n",
    "\n",
    "    # Configure ticks - approximately 4 ticks on each axis\n",
    "    ax.xticks = 0:10:100\n",
    "    ax.yticks = 0:10:100\n",
    "\n",
    "    # Add percentage signs to ticks\n",
    "    ax.xtickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "    ax.ytickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "\n",
    "    ## Add text for miscalibration area\n",
    "    #text!(ax, \"miscalc. area = $(round(area, digits=3))\",\n",
    "    #    position=(8, 2),\n",
    "    #    align=(:left, :bottom)\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_calibration_plot(alpha_refs,predicted_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_miscalibration_area(alpha_refs, predicted_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.010918336035923657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = jldopen(\"./spencer_clustering.jld2\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = f[\"K10_bandwidth10\"][\"centroids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function alt_normdists2centers(feature_vec, centroids)\n",
    "    dists = mapslices(x->Distances.euclidean(feature_vec,x), centroids, dims=1)\n",
    "    normed_dists = dists ./ sum(dists)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = StatsAPI.fit(MultivariateStats.PCA, std_mean_train_features; mean=0)\n",
    "\n",
    "pca_std_train_features = StatsAPI.predict(M1, std_mean_train_features)\n",
    "pca_std_test_features = StatsAPI.predict(M1, std_mean_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist2centers = mapslices(x->reshape(alt_normdists2centers(x,centroids),:,1), pca_std_train_features; dims=1)\n",
    "train_assignments = vec(mapslices(x->argmax(x), train_dist2centers; dims=1))\n",
    "num_inclusters = [length(findall(==(i), train_assignments)) for i in 1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
