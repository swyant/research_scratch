{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6230bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import torch\n",
    "from mace.modules.models import ScaleShiftMACE\n",
    "from mace.tools.scatter import scatter_sum\n",
    "from mace.modules.utils import get_outputs, prepare_graph\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# I truly doubt monkey-patching in this way will work given the torch jit compile, but who knows\n",
    "def fixed_scaleshift_forward(\n",
    "        self,\n",
    "        data: Dict[str, torch.Tensor],\n",
    "        training: bool = False,\n",
    "        compute_force: bool = True,\n",
    "        compute_virials: bool = False,\n",
    "        compute_stress: bool = False,\n",
    "        compute_displacement: bool = False,\n",
    "        compute_hessian: bool = False,\n",
    "        compute_edge_forces: bool = False,\n",
    "        compute_atomic_stresses: bool = False,\n",
    "        lammps_mliap: bool = False,\n",
    "    ) -> Dict[str, Optional[torch.Tensor]]:\n",
    "        # Setup\n",
    "        ctx = prepare_graph(\n",
    "            data,\n",
    "            compute_virials=compute_virials,\n",
    "            compute_stress=compute_stress,\n",
    "            compute_displacement=compute_displacement,\n",
    "            lammps_mliap=lammps_mliap,\n",
    "        )\n",
    "\n",
    "        is_lammps = ctx.is_lammps\n",
    "        num_atoms_arange = ctx.num_atoms_arange.to(torch.int64)\n",
    "        num_graphs = ctx.num_graphs\n",
    "        displacement = ctx.displacement\n",
    "        positions = ctx.positions\n",
    "        vectors = ctx.vectors\n",
    "        lengths = ctx.lengths\n",
    "        cell = ctx.cell\n",
    "        node_heads = ctx.node_heads.to(torch.int64)\n",
    "        interaction_kwargs = ctx.interaction_kwargs\n",
    "        lammps_natoms = interaction_kwargs.lammps_natoms\n",
    "        lammps_class = interaction_kwargs.lammps_class\n",
    "\n",
    "        # Atomic energies\n",
    "        node_e0 = self.atomic_energies_fn(data[\"node_attrs\"])[\n",
    "            num_atoms_arange, node_heads\n",
    "        ]\n",
    "        e0 = scatter_sum(\n",
    "            src=node_e0, index=data[\"batch\"], dim=0, dim_size=num_graphs\n",
    "        ).to(\n",
    "            vectors.dtype\n",
    "        )  # [n_graphs, num_heads]\n",
    "\n",
    "        # Embeddings\n",
    "        node_feats = self.node_embedding(data[\"node_attrs\"])\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        edge_feats, cutoff = self.radial_embedding(\n",
    "            lengths, data[\"node_attrs\"], data[\"edge_index\"], self.atomic_numbers\n",
    "        )\n",
    "\n",
    "        if hasattr(self, \"pair_repulsion\"):\n",
    "            pair_node_energy = self.pair_repulsion_fn(\n",
    "                lengths, data[\"node_attrs\"], data[\"edge_index\"], self.atomic_numbers\n",
    "            )\n",
    "            if is_lammps:\n",
    "                pair_node_energy = pair_node_energy[: lammps_natoms[0]]\n",
    "        else:\n",
    "            pair_node_energy = torch.zeros_like(node_e0)\n",
    "\n",
    "        # Embeddings of additional features\n",
    "        if hasattr(self, \"joint_embedding\"):\n",
    "            embedding_features: Dict[str, torch.Tensor] = {}\n",
    "            for name, _ in self.embedding_specs.items():\n",
    "                embedding_features[name] = data[name]\n",
    "            node_feats += self.joint_embedding(\n",
    "                data[\"batch\"],\n",
    "                embedding_features,\n",
    "            )\n",
    "            if hasattr(self, \"embedding_readout\"):\n",
    "                embedding_node_energy = self.embedding_readout(\n",
    "                    node_feats, node_heads\n",
    "                ).squeeze(-1)\n",
    "                embedding_energy = scatter_sum(\n",
    "                    src=embedding_node_energy,\n",
    "                    index=data[\"batch\"],\n",
    "                    dim=0,\n",
    "                    dim_size=num_graphs,\n",
    "                )\n",
    "                e0 += embedding_energy\n",
    "\n",
    "        # Interactions\n",
    "        node_es_list = [pair_node_energy]\n",
    "        node_feats_list: List[torch.Tensor] = []\n",
    "\n",
    "        for i, (interaction, product) in enumerate(\n",
    "            zip(self.interactions, self.products)\n",
    "        ):\n",
    "            node_attrs_slice = data[\"node_attrs\"]\n",
    "            if is_lammps and i > 0:\n",
    "                node_attrs_slice = node_attrs_slice[: lammps_natoms[0]]\n",
    "            node_feats, sc = interaction(\n",
    "                node_attrs=node_attrs_slice,\n",
    "                node_feats=node_feats,\n",
    "                edge_attrs=edge_attrs,\n",
    "                edge_feats=edge_feats,\n",
    "                edge_index=data[\"edge_index\"],\n",
    "                cutoff=cutoff,\n",
    "                first_layer=(i == 0),\n",
    "                lammps_class=lammps_class,\n",
    "                lammps_natoms=lammps_natoms,\n",
    "            )\n",
    "            if is_lammps and i == 0:\n",
    "                node_attrs_slice = node_attrs_slice[: lammps_natoms[0]]\n",
    "            node_feats = product(\n",
    "                node_feats=node_feats, sc=sc, node_attrs=node_attrs_slice\n",
    "            )\n",
    "            node_feats_list.append(node_feats)\n",
    "\n",
    "        for i, readout in enumerate(self.readouts):\n",
    "            feat_idx = -1 if len(self.readouts) == 1 else i\n",
    "            node_es_list.append(\n",
    "                readout(node_feats_list[feat_idx], node_heads)[\n",
    "                    num_atoms_arange, node_heads\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        node_feats_out = torch.cat(node_feats_list, dim=-1)\n",
    "        node_inter_es = torch.sum(torch.stack(node_es_list, dim=0), dim=0)\n",
    "        node_inter_es = self.scale_shift(node_inter_es, node_heads)\n",
    "        inter_e = scatter_sum(node_inter_es, data[\"batch\"], dim=-1, dim_size=num_graphs)\n",
    "\n",
    "        total_energy = e0 + inter_e\n",
    "\n",
    "        # No float64 casting\n",
    "        node_energy = node_e0.clone() + node_inter_es.clone()\n",
    "\n",
    "        forces, virials, stress, hessian, edge_forces = get_outputs(\n",
    "            energy=inter_e,\n",
    "            positions=positions,\n",
    "            displacement=displacement,\n",
    "            vectors=vectors,\n",
    "            cell=cell,\n",
    "            training=training,\n",
    "            compute_force=compute_force,\n",
    "            compute_virials=compute_virials,\n",
    "            compute_stress=compute_stress,\n",
    "            compute_hessian=compute_hessian,\n",
    "            compute_edge_forces=compute_edge_forces or compute_atomic_stresses,\n",
    "        )\n",
    "\n",
    "        atomic_virials: Optional[torch.Tensor] = None\n",
    "        atomic_stresses: Optional[torch.Tensor] = None\n",
    "        if compute_atomic_stresses and edge_forces is not None:\n",
    "            atomic_virials, atomic_stresses = get_atomic_virials_stresses(\n",
    "                edge_forces=edge_forces,\n",
    "                edge_index=data[\"edge_index\"],\n",
    "                vectors=vectors,\n",
    "                num_atoms=positions.shape[0],\n",
    "                batch=data[\"batch\"],\n",
    "                cell=cell,\n",
    "            )\n",
    "        return {\n",
    "            \"energy\": total_energy,\n",
    "            \"node_energy\": node_energy,\n",
    "            \"interaction_energy\": inter_e,\n",
    "            \"forces\": forces,\n",
    "            \"edge_forces\": edge_forces,\n",
    "            \"virials\": virials,\n",
    "            \"stress\": stress,\n",
    "            \"atomic_virials\": atomic_virials,\n",
    "            \"atomic_stresses\": atomic_stresses,\n",
    "            \"hessian\": hessian,\n",
    "            \"displacement\": displacement,\n",
    "            \"node_feats\": node_feats_out,\n",
    "        }\n",
    "\n",
    "ScaleShiftMACE.forward = fixed_scaleshift_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab406fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "small_f32_model = torch.load(f=\"../models/small_float32.model\", map_location=\"mps\", weights_only=False)\n",
    "import eval_my_mlips.utils as emu\n",
    "from mace.calculators import MACECalculator\n",
    "data_dir = \"/Users/swyant/cesmix/datasets/HfO2_cesmix/april_2025_data_mv/processed_and_split/\"\n",
    "reference_configs = emu.load_configurations(data_dir)\n",
    "sample_atoms = reference_configs[\"HfOx_test_0K\"][1]\n",
    "calc_f32 = MACECalculator(models=[small_f32_model,],device=\"mps\", default_dtype=\"float32\")\n",
    "sample_atoms.calc = calc_f32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_atoms.get_potential_energy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ae940",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_configs = emu.compute_predictions(reference_configs, calc_f32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b13b3",
   "metadata": {},
   "source": [
    "It works but it's just as slow, maybe slower? (Maybe no torch compilation. Or running on MPS on my poor little M1 isn't worth it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27634e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
