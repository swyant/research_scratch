{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "\n",
    "using PotentialLearning, InteratomicPotentials\n",
    "using Unitful\n",
    "using Random\n",
    "using AtomsBase\n",
    "using DelimitedFiles\n",
    "using Statistics: mean, var\n",
    "using StatsBase\n",
    "using Clustering, Distances\n",
    "using Trapz\n",
    "using LinearAlgebra: Symmetric, eigen\n",
    "\n",
    "#using CairoMakie CairoMakie.activate!()\n",
    "using GLMakie; GLMakie.activate!(inline=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_miscalibration_area(expected_ps, observed_ps)\n",
    "    area = 0.0\n",
    "    #for i in 2:length(expected_ps)-1\n",
    "    #    trap = abs(trapz(expected_ps[i-1:i+1], observed_ps[i-1:i+1]) -\n",
    "    #             trapz(expected_ps[i-1:i+1], expected_ps[i-1:i+1]))\n",
    "    for i in 2:length(expected_ps)\n",
    "        trap = abs(trapz(expected_ps[i-1:i], observed_ps[i-1:i]) -\n",
    "                 trapz(expected_ps[i-1:i], expected_ps[i-1:i]))\n",
    "        area += trap\n",
    "    end\n",
    "    area\n",
    "end\n",
    "\n",
    "# converted from Medford jupyter notebook via Claude\n",
    "function make_calibration_plot(expected_ps, observed_ps; width=600)\n",
    "    # Convert to percentages\n",
    "    expected_ps = expected_ps .* 100\n",
    "    observed_ps = observed_ps .* 100\n",
    "\n",
    "    fig = Figure(resolution=(width, width))\n",
    "    ax = Axis(fig[1, 1],\n",
    "        aspect=DataAspect(),\n",
    "        xlabel=\"Expected conf. level\",\n",
    "        ylabel=\"Observed conf. level\",\n",
    "        limits=(0, 100, 0, 100)\n",
    "    )\n",
    "\n",
    "    # Main line\n",
    "    lines!(ax, expected_ps, observed_ps)\n",
    "\n",
    "    # Diagonal reference line\n",
    "    lines!(ax, expected_ps, expected_ps, linestyle=:dash, alpha=0.4)\n",
    "\n",
    "    # Filled area between curves\n",
    "    band!(ax, expected_ps, expected_ps, observed_ps, color=(:blue, 0.2))\n",
    "\n",
    "    # Configure ticks - approximately 4 ticks on each axis\n",
    "    ax.xticks = 0:10:100\n",
    "    ax.yticks = 0:10:100\n",
    "\n",
    "    # Add percentage signs to ticks\n",
    "    ax.xtickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "    ax.ytickformat = xs -> [\"$(Int(x))%\" for x in xs]\n",
    "\n",
    "    ## Add text for miscalibration area\n",
    "    #text!(ax, \"miscalc. area = $(round(area, digits=3))\",\n",
    "    #    position=(8, 2),\n",
    "    #    align=(:left, :bottom)\n",
    "    #)\n",
    "\n",
    "    return fig\n",
    "end\n",
    "\n",
    "# Claude\n",
    "function parity_plot(etest_ref, etest_pred, qhat_scored;\n",
    "                     title=\"Parity Plot\",\n",
    "                     xlabel=\"Reference Values\",\n",
    "                     ylabel=\"Predicted Values\",\n",
    "                     figsize=(600, 600))\n",
    "    # Create figure and axis\n",
    "    fig = Figure(size=figsize)\n",
    "    ax = Axis(fig[1, 1],\n",
    "    title=title,\n",
    "    xlabel=xlabel,\n",
    "    ylabel=ylabel,\n",
    "    limits = (-5.0,-4.0,-5.0,-4.0))\n",
    "\n",
    "    # Calculate min and max for setting plot limits\n",
    "    min_val = min(minimum(etest_pred), minimum(etest_ref))\n",
    "    max_val = max(maximum(etest_pred), maximum(etest_ref))\n",
    "\n",
    "    # Add diagonal reference line\n",
    "    lines!(ax, [min_val, max_val], [min_val, max_val],\n",
    "    color=:red,\n",
    "    linestyle=:dash,\n",
    "    label=\"Perfect Prediction\")\n",
    "\n",
    "    # Plot scatter with error bars\n",
    "    errorbars!(ax, etest_ref, etest_pred, qhat_scored,\n",
    "    whiskerwidth=1,  # Width of error bar caps\n",
    "    color=:cyan3)\n",
    "\n",
    "    # Scatter plot of points\n",
    "    scatter!(ax, etest_ref, etest_pred,\n",
    "    color=:teal,\n",
    "    markersize=10)\n",
    "\n",
    "    # Set equal aspect ratio\n",
    "    #ax.aspect = DataAspect()\n",
    "\n",
    "    # Add legend\n",
    "    axislegend(ax)\n",
    "\n",
    "    return fig\n",
    "end\n",
    "\n",
    "function uncertainty_vs_residuals(uncertainty, residuals;\n",
    "                                 title  =\"Uncertainty vs. residuals\",\n",
    "                                 xlabel = \"Distance\",\n",
    "                                 ylabel = \"Residuals\",\n",
    "                                 figsize = (600,600))\n",
    "    fig = Figure(size=figsize)\n",
    "    ax  = Axis(fig[1,1],\n",
    "               title=title,\n",
    "               xlabel=xlabel,\n",
    "               ylabel=ylabel)\n",
    "\n",
    "    hlines!(ax, 0.0, color=:red, linestyle=:dash)\n",
    "\n",
    "    scatter!(ax, uncertainty, residuals, markersize=1)\n",
    "\n",
    "    #ax.aspect=DataAspect()\n",
    "    fig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ace = ACE(species           = [:C,:H,:O,:N],\n",
    "          body_order        = 3,\n",
    "          polynomial_degree = 10,\n",
    "          wL                = 2.0,\n",
    "          csp               = 1.0,\n",
    "          r0                = 1.43,\n",
    "          rcutoff           = 4.4 )\n",
    "lb = LBasisPotential(ace)\n",
    "length(ace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_file = \"../files/QM9/qm9_fullset_alldata.xyz\"\n",
    "raw_data = load_data(qm9_file, ExtXYZ(u\"eV\", u\"Å\"))\n",
    "raw_data = DataSet([config for config in raw_data if !(:F in atomic_symbol(get_system(config)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing structures with Fluorine results in 1,1923 fewer configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_perm_idxs = readdlm(\"./primary_permutation.txt\", Int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_train = 120_001\n",
    "possible_training_idxs = master_perm_idxs[1:max_num_train]\n",
    "possible_test_idxs = master_perm_idxs[max_num_train+1:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 40_000\n",
    "train_idxs = possible_training_idxs[1:num_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_file = \"dummy_param.txt\"\n",
    "\n",
    "#_AtWA, _AtWb = PotentialLearning.ooc_learn_eonly!(lb, raw_data[train_idxs];symmetrize=false, λ=0.01, pbar=false)\n",
    "#\n",
    "#open(param_file, \"w\") do io\n",
    "#    writedlm(io, lb.β)\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.β .= readdlm(\"qm9_4elem_3body_poly10_fit40K.txt\", Float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etest_ref = get_all_energies(raw_data[possible_test_idxs])\n",
    "\n",
    "etest_local_descrs = compute_local_descriptors(raw_data[possible_test_idxs],lb.basis)\n",
    "ds_test = DataSet(raw_data[possible_test_idxs] .+ etest_local_descrs)\n",
    "etest_pred = get_all_energies(ds_test,lb)\n",
    "\n",
    "num_atoms_test = length.(get_system.(raw_data[possible_test_idxs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show e_mae, e_rmse, e_rsq = calc_metrics(etest_pred,etest_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e_mae, e_rmse, e_rsq) = calc_metrics(etest_pred, etest_ref) = (0.04422691459021048, 0.06343019540788734, 0.9999609908582189)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes about computing the distance vector: \n",
    "- feature vector for each config is averaged over atoms (not summed)\n",
    "- feature vector is standardized when generating the k-means cluster\n",
    "- does appear to be using Euclidean distance with k-means (they pass \"Minkowski\", which seems to default to p=2)\n",
    "- when getting the final distance metric, they take the average distance between all cluster centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_mean_features(ds)\n",
    "    mean_feature_perconfig = Vector{Float64}[]\n",
    "    for (i,config) in enumerate(ds)\n",
    "        if i % 100 == 0\n",
    "            println(i)\n",
    "        end\n",
    "        mean_feature = mean(InteratomicPotentials.compute_local_descriptors(get_system(config), lb.basis))\n",
    "        push!(mean_feature_perconfig,mean_feature)\n",
    "    end\n",
    "\n",
    "    reduce(hcat,mean_feature_perconfig)\n",
    "end\n",
    "\n",
    "# can either average over the distances between centers, or find the minimum\n",
    "# could also vary the distance here...\n",
    "function heuristic_uncertainty1(mean_feature_vec, km)\n",
    "    #dist = mean(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "    dist = minimum(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "end\n",
    "\n",
    "function heuristic_uncertainty2(mean_feature_vec, km)\n",
    "    dist = mean(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "    #dist = minimum(mapslices(x->Distances.euclidean(mean_feature_vec,x), km.centers, dims=1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train_features = compute_mean_features(raw_data[train_idxs])\n",
    "mean_test_features  = compute_mean_features(raw_data[possible_test_idxs])\n",
    "\n",
    "dt = StatsBase.fit(ZScoreTransform, mean_train_features, dims=2)\n",
    "std_mean_train_features = StatsBase.transform(dt,mean_train_features)\n",
    "std_mean_test_features = StatsBase.transform(dt,mean_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Symmetric(mean(di*di' for di in eachrow(std_mean_train_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors = 20\n",
    "km = kmeans(std_mean_train_features, num_neighbors, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "km_10 = kmeans(std_mean_train_features, 10, distance=Distances.Euclidean(), rng=Xoshiro(1))\n",
    "#km_50  = kmeans(std_mean_test_features, 50, distance=Distances.Euclidean(), rng=Xoshiro(1)) #Ah i Fucked up here\n",
    "km_50  = kmeans(std_mean_train_features, 50, distance=Distances.Euclidean(), rng=Xoshiro(1)) #Ah i Fucked up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(stdout, \"text/plain\", pairwise(Distances.Euclidean(), km.centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_distances1 = mapslices(vec -> heuristic_uncertainty1(vec, km), std_mean_test_features, dims=1)\n",
    "test_feature_distances2 = mapslices(vec -> heuristic_uncertainty2(vec, km), std_mean_test_features, dims=1)\n",
    "\n",
    "test_feature_distances3 = mapslices(vec -> heuristic_uncertainty1(vec, km_10), std_mean_test_features, dims=1)\n",
    "test_feature_distances4 = mapslices(vec -> heuristic_uncertainty1(vec, km_50), std_mean_test_features, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xlims!(0,150)\n",
    "hist(test_feature_distances4[1,:], bins=1000)\n",
    "#xlims!(0,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlims!(0,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_distances = test_feature_distances4\n",
    "fraction_calib = 0.1\n",
    "peratom = true\n",
    "alpha = 0.05\n",
    "num_calib = floor(Int64, fraction_calib*length(possible_test_idxs))\n",
    "num_test = length(possible_test_idxs) - num_calib\n",
    "\n",
    "#idxs_wrt_test = Random.randperm(length(possible_test_idxs))\n",
    "idxs_wrt_test = collect(1:length(possible_test_idxs))\n",
    "\n",
    "calib_idxs_wrt_test = idxs_wrt_test[1:num_calib]\n",
    "test_idxs_wrt_test = idxs_wrt_test[num_calib+1:end]\n",
    "\n",
    "if !peratom\n",
    "    calib_scores = abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test]) ./ test_feature_distances[calib_idxs_wrt_test]\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test])\n",
    "\n",
    "else\n",
    "    calib_scores = ( abs.(etest_pred[calib_idxs_wrt_test] .- etest_ref[calib_idxs_wrt_test])\n",
    "                    ./ num_atoms_test[calib_idxs_wrt_test] ./ test_feature_distances[calib_idxs_wrt_test] )\n",
    "    test_abs_residuals = abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./ num_atoms_test[test_idxs_wrt_test]\n",
    "\n",
    "end\n",
    "\n",
    "q_hat = quantile(calib_scores, ceil((num_calib+1)*(1-alpha))/num_calib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Medford paper takes as a quantity of interest as the energy normalized by the number of atoms, rather than the raw energy. I suspect that it doesn't make that much of a difference for this dataset since the number of atoms are pretty similar, but for very big differences I'm sure it probably starts to matter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qhat_scores = q_hat*test_feature_distances[test_idxs_wrt_test]\n",
    "coverage = sum(test_abs_residuals .> qhat_scores) / num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = parity_plot(etest_ref[test_idxs_wrt_test]./ num_atoms_test[test_idxs_wrt_test],etest_pred[test_idxs_wrt_test]./ num_atoms_test[test_idxs_wrt_test],qhat_scores)\n",
    "# Should probably do this as a residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_residuals = (etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./num_atoms_test[test_idxs_wrt_test]\n",
    "#hist(abs.(etest_pred[test_idxs_wrt_test] .- etest_ref[test_idxs_wrt_test]) ./num_atoms_test[test_idxs_wrt_test],bins=100)\n",
    "hist(test_residuals,bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = Figure(size=(600,600))\n",
    "#ax  = Axis(fig[1,1],\n",
    "#title=\"Residuals vs distances\",\n",
    "#xlabel=\"Distances\",\n",
    "#ylabel=\"Residuals\",\n",
    "#limits=(0,100,-0.005,0.03))\n",
    "#hlines!(ax, 0.0, color=:red, linestyle=:dash)\n",
    "##scatter!(ax, test_feature_distances[test_idxs_wrt_test], test_abs_residuals, markersize=1)\n",
    "#scatter!(ax, test_feature_distances2[test_idxs_wrt_test], test_abs_residuals, markersize=5)\n",
    "##ax.aspect=DataAspect()\n",
    "\n",
    "function uncertainty_vs_residuals(uncertainty, residuals;\n",
    "                                  title  =\"Uncertainty vs. residuals\",\n",
    "                                  xlabel = \"Distance\",\n",
    "                                  ylabel = \"Residuals\",\n",
    "                                  figsize = (600,600))\n",
    "\n",
    "    fig = Figure(size=(600,600))\n",
    "    ax  = Axis(fig[1,1],\n",
    "    title=\"Residuals vs distances\",\n",
    "    xlabel=\"Distances\",\n",
    "    ylabel=\"Residuals\",\n",
    "    limits=(0,100,-0.01,0.03))\n",
    "\n",
    "    hlines!(ax, 0.0, color=:red, linestyle=:dash)\n",
    "\n",
    "    scatter!(ax, uncertainty, residuals, markersize=5)\n",
    "\n",
    "    #ax.aspect=DataAspect()\n",
    "    fig\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "#scatter(test_feature_distances[test_idxs_wrt_test], test_abs_residuals)\n",
    "uncertainty_vs_residuals(test_feature_distances[test_idxs_wrt_test], test_abs_residuals) # idk why this isn't plotting correctly\n",
    "#fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = test_feature_distances4[test_idxs_wrt_test]\n",
    "yvals = test_abs_residuals\n",
    "A = hcat(ones(length(xvals)), xvals)\n",
    "coeffs = A \\ yvals\n",
    "\n",
    "y_pred = coeffs[2] .*xvals .+ coeffs[1]\n",
    "\n",
    "ymean = mean(yvals)\n",
    "ss_total = sum((yvals .- ymean).^2)\n",
    "ss_residual = sum((yvals .- y_pred).^2)\n",
    "r_squared = 1 - (ss_residual / ss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_res = test_abs_residuals\n",
    "@show length(abs_res)\n",
    "for bin_start in 0.000:0.001:0.015\n",
    "    low = bin_start\n",
    "    high = bin_start + 0.001\n",
    "    idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "    local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "    println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "end\n",
    "low = 0.012\n",
    "high = 0.02\n",
    "idxs = [i for i in eachindex(abs_res) if abs_res[i] >= low && abs_res[i] < high]\n",
    "local_coverage = 1-sum(abs_res[idxs] .> qhat_scores[idxs])/length(idxs)\n",
    "println(\"$(low)-$(high) : $(length(idxs)) configs with coverage $(local_coverage)\")\n",
    "\n",
    "local_coverage = 1 - sum(abs_res .> qhat_scores)/length(abs_res)\n",
    "println(\"overall coverage is $(local_coverage)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness = 2*mean(qhat_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var(qhat_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so I think the sharpness here should be doubled, because qhat_scores is only one side of the symmetric uncertainty. \n",
    "Regardless for 1-alpha =0.68, the mean here is 0.0029635544980581414 or about 3 meV (per atom). \n",
    "In contrast https://github.com/medford-group/conformal_prediction_in_latent_space/blob/master/uncertainty/analyze_conformal_feature.ipynb is \n",
    "np.mean(test_uncertainty1) = 1.6777220081885507 meV\n",
    "\n",
    "So I'm getting a sharpness almost 2x as large. \n",
    "\n",
    "They plot a histogram (though it doesn't show up) of the their equivalent of qhat_scores. I don't really want to try to set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(qhat_scores,bins=1000)\n",
    "# I'm not sure, quantitatively, what constitutes being sufficiently adaptive.\n",
    "# Ultimately it seems dependent on the dataset in addition to the score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_complements = collect(range(0.01,0.99,step=0.01))\n",
    "alpha_refs = 1 .- alpha_complements\n",
    "\n",
    "alpha_refs = collect(range(0.01,0.99,step=0.01))\n",
    "\n",
    "predicted_alphas = Float64[]\n",
    "#for ac in alpha_complements\n",
    "#    alpha = 1-ac\n",
    "for alpha in alpha_refs\n",
    "    qh = quantile(calib_scores, clamp(ceil((num_calib+1)*(1-alpha))/num_calib, 0.0, 1.0))\n",
    "\n",
    "    qh_scores = qh*test_feature_distances[test_idxs_wrt_test]\n",
    "    predicted_alpha = sum(test_abs_residuals .> qh_scores) / num_test\n",
    "    push!(predicted_alphas, predicted_alpha)\n",
    "end\n",
    "\n",
    "# I feel like I plot the reverse of what I want like 70 is 30 and 30 is 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_miscalibration_area(alpha_refs, predicted_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_calibration_plot(alpha_refs,predicted_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to fix miscalibration plot what is 70% should be 30%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
